{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The note book file is run in google collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the toolkit in order to access the dataset\n",
    "# pip install delitoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary package\n",
    "import delitoolkit\n",
    "from delitoolkit.delidata import DeliData\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "delidata_corpus = DeliData()\n",
    "groups = list(delidata_corpus.corpus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTITION THE DATASET INTO GROUPS BASED ON THE NUMBER OF USERS\n",
    "\n",
    "user2 = set()\n",
    "user3 = set()\n",
    "user4 = set()\n",
    "user5 = set()\n",
    "\n",
    "conv0to20 = set()\n",
    "conv20to40 = set()\n",
    "conv40up = set()\n",
    "\n",
    "def getUserCount(string):\n",
    "  before_and = string.split('&&')[0]\n",
    "  words_before_and = before_and.split(',')\n",
    "  count_words = len(words_before_and)\n",
    "  #since SYSTEM is included in count reduce it\n",
    "  return count_words - 1\n",
    "\n",
    "def addToList(count, group_id):\n",
    "  if count == 2:\n",
    "    if group_id not in user2:\n",
    "      user2.add(group_id)\n",
    "\n",
    "  elif count == 3:\n",
    "    if group_id not in user3:\n",
    "      user3.add(group_id)\n",
    "\n",
    "  elif count == 4:\n",
    "    if group_id not in user4:\n",
    "      user4.add(group_id)\n",
    "\n",
    "  else:\n",
    "    if group_id not in user5:\n",
    "      user5.add(group_id)\n",
    "\n",
    "def addConversationToList(count, group_id):\n",
    "  if count < 20:\n",
    "    if group_id not in conv0to20:\n",
    "      conv0to20.add(group_id)\n",
    "\n",
    "  elif 20 <= count < 40:\n",
    "    if group_id not in conv20to40:\n",
    "      conv20to40.add(group_id)\n",
    "\n",
    "  else:\n",
    "    if group_id not in conv40up:\n",
    "      conv40up.add(group_id)\n",
    "\n",
    "delidata_corpus = DeliData()\n",
    "groups = list(delidata_corpus.corpus.keys())\n",
    "for group in groups:\n",
    "  conversation_length = len(delidata_corpus.corpus[group])\n",
    "  addConversationToList(conversation_length, group)\n",
    "  for m in delidata_corpus.corpus[group]:\n",
    "    if m['origin'] == 'SYSTEM':\n",
    "      userCount = getUserCount(m['original_text'])\n",
    "      addToList(userCount, m['group_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifing the grouppings\n",
    "print(len(conv0to20) + len(conv20to40) + len(conv40up))\n",
    "print(len(user2) + len(user3) + len(user4) + len(user5))\n",
    "print(\"conv0-20:\", len(conv0to20), \"conv20-40:\",len(conv20to40) ,\"conv40+:\", len(conv40up))\n",
    "print(\"convWith2Deliberators:\", len(user2) , \"convWith3Deliberators:\", len(user3) , \"convWith4Deliberators:\", len(user4) , \"convWith5Deliberators:\", len(user5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating classes to process the dataset and store information realted to each conversation\n",
    "class GroupDetails:\n",
    "  def __init__(\n",
    "    self,\n",
    "    number_of_conversations,\n",
    "    number_of_participants,\n",
    "    message_per_user,\n",
    "    submission_count_per_user,\n",
    "    submission_per_user,\n",
    "    consensus_,\n",
    "    idle_users\n",
    "    ):\n",
    "    self.number_of_conversations = number_of_conversations\n",
    "    self.number_of_participants = number_of_participants\n",
    "    self.message_per_user = message_per_user\n",
    "    self.submission_count_per_user = submission_count_per_user\n",
    "    self.submission_per_user = submission_per_user\n",
    "    self.consensus_ = consensus_\n",
    "    self.idle_users = idle_users\n",
    "\n",
    "  def __str__(self):\n",
    "      return (f\"GroupDetails(number_of_conversations={self.number_of_conversations}, \\n\"\n",
    "              f\"number_of_participants={self.number_of_participants}, \\n\"\n",
    "              f\"message_per_user={self.message_per_user}, \\n\"\n",
    "              f\"submission_count_per_user={self.submission_count_per_user}, \\n\"\n",
    "              f\"submission_per_user={self.submission_per_user}, \\n\"\n",
    "              f\"consensus_={self.consensus_}, \\n\"\n",
    "              f\"idle_users={self.idle_users})\")\n",
    "\n",
    "class GroupStorage:\n",
    "    def __init__(self):\n",
    "      self.storage_ = {}\n",
    "\n",
    "    def addToStorage(self, group_id):\n",
    "      group_converstaion = delidata_corpus.corpus[group_id]\n",
    "      length_of_converstaion = len(group_converstaion) - 1\n",
    "      idle_user_count = 0\n",
    "      consensus = 0\n",
    "\n",
    "      def getStats():\n",
    "        message_count = {}\n",
    "        submit_count = {}\n",
    "        user_submission = {}\n",
    "        num_user = 0\n",
    "        for m in group_converstaion:\n",
    "          m_type, origin_, original_text = m['message_type'], m['origin'], m['original_text']\n",
    "          if m_type == \"INITIAL\":\n",
    "            num_user = getUserCount(original_text)\n",
    "          elif m_type == \"MESSAGE\":\n",
    "             count = message_count.get(origin_, 0)\n",
    "             message_count[origin_] = count +  1\n",
    "          else:\n",
    "            count =  submit_count.get(origin_, 0)\n",
    "            submit_count[origin_] = count + 1\n",
    "            user_submission[origin_] = original_text\n",
    "        return message_count, submit_count, user_submission, num_user\n",
    "\n",
    "      def getConsensus(user_submission, num_user):\n",
    "          responseCount = dict()\n",
    "          for k,v in user_submission.items():\n",
    "            count = responseCount.get(v, 0)\n",
    "            responseCount[v] = count + 1\n",
    "          max = 0\n",
    "          for k,v in responseCount.items():\n",
    "              if v > max:\n",
    "                  max = v\n",
    "              # print(v, \" number of users selected \", k)\n",
    "              # print(int(v)/num_user * 100, \" % agreement\")\n",
    "          return float(int(max)/num_user)\n",
    "\n",
    "      message_count, submit_count, user_submission, num_user = getStats()\n",
    "      if len(user_submission) == 0:\n",
    "        print(\"ZERO\", group_id)\n",
    "      else:\n",
    "        consensus = getConsensus(user_submission, num_user)\n",
    "\n",
    "      count = 0\n",
    "      for k,v in message_count.items():\n",
    "        count += 1\n",
    "      if count != num_user:\n",
    "        idle_user_count = num_user - count\n",
    "\n",
    "      group_details = GroupDetails(\n",
    "          length_of_converstaion,\n",
    "          num_user,\n",
    "          message_count,\n",
    "          submit_count,\n",
    "          user_submission,\n",
    "          consensus,\n",
    "          idle_user_count\n",
    "      )\n",
    "      self.storage_[group_id] = group_details\n",
    "\n",
    "class Group:\n",
    "  def __init__(self, array, group_name):\n",
    "    self.array_ = array\n",
    "    self.group_name = group_name\n",
    "    self.group_storage = GroupStorage()\n",
    "\n",
    "  def computeMessageStat(self):\n",
    "    count = 0\n",
    "    for group_id in self.array_:\n",
    "      count += 1\n",
    "      self.group_storage.addToStorage(group_id)\n",
    "\n",
    "  def drawBoxPlot(self):\n",
    "    if self.group_name == \"A\":\n",
    "        categoryName = \"A, [5-20) number of deliberation\"\n",
    "    if self.group_name == \"B\":\n",
    "        categoryName = \"B, [20-40) number of deliberation\"\n",
    "    if self.group_name == \"C\":\n",
    "        categoryName = \"C, 40+ number of deliberation\"\n",
    "    if self.group_name == \"#2\":\n",
    "        categoryName = \"Number of users 2\"\n",
    "    if self.group_name == \"#3\":\n",
    "        categoryName = \"NUmber of users 3\"\n",
    "    if self.group_name == \"#4\":\n",
    "        categoryName = \"Number of users 4\"\n",
    "    if self.group_name == \"#5\":\n",
    "        categoryName = \"Number of users 5\"\n",
    "      \n",
    "    numOfConverstaions= []\n",
    "    for k,v in self.group_storage.storage_.items():\n",
    "      numOfConverstaions.append(v.number_of_conversations)\n",
    "    plt.boxplot(numOfConverstaions)\n",
    "    plt.title('Number of Deliberations for category ' + categoryName)\n",
    "    plt.ylabel('No of deliberations')\n",
    "    plt.show()\n",
    "\n",
    "  def drawBarPlot(self):\n",
    "    if self.group_name == \"A\":\n",
    "        categoryName = \"A, [5-20) number of deliberation\"\n",
    "    if self.group_name == \"B\":\n",
    "        categoryName = \"B, [20-40) number of deliberation\"\n",
    "    if self.group_name == \"C\":\n",
    "        categoryName = \"C, 40+ number of deliberation\"\n",
    "    if self.group_name == \"#2\":\n",
    "        categoryName = \"Number of users 2\"\n",
    "    if self.group_name == \"#3\":\n",
    "        categoryName = \"NUmber of users 3\"\n",
    "    if self.group_name == \"#4\":\n",
    "        categoryName = \"Number of users 4\"\n",
    "    if self.group_name == \"#5\":\n",
    "        categoryName = \"Number of users 5\"\n",
    "      \n",
    "    numOfConverstaions= []\n",
    "    fileName = []\n",
    "    majority = []\n",
    "    count = 1\n",
    "    for k,v in self.group_storage.storage_.items():\n",
    "      if v.number_of_conversations > 90:\n",
    "        print(k)\n",
    "      numOfConverstaions.append(v.number_of_conversations)\n",
    "      fileName.append(count)\n",
    "      count += 1\n",
    "      majority.append(v.consensus_ * 100)\n",
    "    \n",
    "    if count > 20:\n",
    "      numOfConverstaions = numOfConverstaions[0:21]\n",
    "      fileName = fileName[:21]\n",
    "      majority = majority[:21]\n",
    "      \n",
    "    users = fileName\n",
    "    y1 = majority\n",
    "    y2 = numOfConverstaions\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.bar(users, y1, color='brown', alpha=0.5, align='center')\n",
    "    ax1.set_ylabel('Majority Agreement %', color='b')\n",
    "    ax1.set_xlabel('Converstaion Name', color='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax2.plot(users, y2, color='orange', marker='o')\n",
    "    ax2.set_ylabel('Number of deliberations', color='b')\n",
    "\n",
    "    ax2.legend(loc='upper left')\n",
    "\n",
    "    plt.title('Category ' + categoryName)\n",
    "    plt.xlabel('Users')\n",
    "\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = Group(list(user2), \"#2\")\n",
    "print(obj2.computeMessageStat())\n",
    "print(len(user2), len(obj2.group_storage.storage_))\n",
    "\n",
    "obj3 = Group(list(user3), \"#3\")\n",
    "print(obj3.computeMessageStat())\n",
    "print(len(user3), len(obj3.group_storage.storage_))\n",
    "\n",
    "obj4 = Group(list(user4), \"#4\")\n",
    "print(obj4.computeMessageStat())\n",
    "print(len(user4), len(obj4.group_storage.storage_))\n",
    "\n",
    "obj5 = Group(list(user5), \"#5\")\n",
    "print(obj5.computeMessageStat())\n",
    "print(len(user5), len(obj5.group_storage.storage_))\n",
    "\n",
    "objConv2 = Group(list(conv0to20), \"A\")\n",
    "objConv4 = Group(list(conv20to40), \"B\")\n",
    "objConv4Plus = Group(list(conv40up), \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually looking at the objctes created\n",
    "for k,v in obj2.group_storage.storage_.items():\n",
    "  print(k,v)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install elasticsearch\n",
    "# pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "LINK = os.getenv('LINK')\n",
    "API_KEY = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(LINK, API_KEY)\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "      print(\"Could not connect to Elastcsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataset to load it into elasticsearch\n",
    "# preprocess the data so that we can do run elastic search on it\n",
    "res = []\n",
    "for group in groups:\n",
    "  temp = {}\n",
    "  temp[\"id\"] = group\n",
    "  message = []\n",
    "  for m in delidata_corpus.corpus[group]:\n",
    "    message.append(m['original_text'])\n",
    "  temp[\"message\"] = message\n",
    "  res.append(temp)\n",
    "\n",
    "print(len(res))\n",
    "\n",
    "with open(\"conv.json\", 'w') as f:\n",
    "    json.dump(res, f, indent=4)  # Use indent for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the document to elastic search\n",
    "import json\n",
    "with open('conv.json', mode='r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    # Index each record in the JSON file\n",
    "    for record in data:\n",
    "        # Use a specific field as the document ID or auto-generate one\n",
    "        # response = es.index(index='my_json_index', id=record['id'], document=record)\n",
    "        response = es.index(index='conv_index', id=record['id'], document=record)\n",
    "        print(f\"Indexed document with ID {record['id']} with response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the dataset for phrases (tolerates subsets)\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match_phrase\": {\n",
    "            \"message\": \"What did you pick\"  # Search for Alice2 in the name array\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the dataset for the exact phrase\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"message.keyword\": \"I think A and 5 are the best choice because ones a vowel ones odd\"  # Search for Alice2 in the name array\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es.search(index='conv_index', body=search_query)\n",
    "print(json.dumps(search_results['hits'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Index Unigram Index\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# mapping for word -> [group-ids]\n",
    "unigram_mapping = defaultdict(list)\n",
    "\n",
    "for group in groups[:1]:\n",
    "  for conv in delidata_corpus.corpus[group]:\n",
    "    # print(conv)\n",
    "    if conv['message_type'] == 'INITIAL' or conv['message_type'] == 'SUBMIT':\n",
    "      continue\n",
    "    sentence = conv['original_text']\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "      unigram_mapping[word].append(group)\n",
    "\n",
    "print(unigram_mapping)\n",
    "\n",
    "\n",
    "# search\n",
    "search_word = \"Hello.\"\n",
    "if search_word in unigram_mapping:\n",
    "  print(\"word \", search_word,  \" in \", unigram_mapping[search_word], \" groups\")\n",
    "else:\n",
    "  print(\"Word not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload file\n",
    "with open('convToUpload.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "dict_list = json.loads(data.replace(\"'\", '\"'))  # Replace single quotes with double quotes for valid JSON\n",
    "# for entry in dict_list:\n",
    "#     print(entry)\n",
    "\n",
    "# classify them\n",
    "group = dict_list[0]['group_id']\n",
    "conversation_length = len(dict_list)\n",
    "addConversationToList(conversation_length, group)\n",
    "for m in dict_list:\n",
    "  if m['origin'] == 'SYSTEM':\n",
    "    userCount = getUserCount(m['original_text'])\n",
    "    addToList(userCount, m['group_id'])\n",
    "\n",
    "# get and store the properties of the uploaded conversation\n",
    "obj4.addAndComputeStat(group, dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the properties of the uploaded conversation\n",
    "\n",
    "# Check to see if this is placed in the group with 4 users\n",
    "assert (\"abc\" in user4) == True\n",
    "assert (\"abc\" in user3) == False\n",
    "assert (\"abc\" in user2) == False\n",
    "\n",
    "# Check to see if this is placed in the group with 40+ number of converstaions\n",
    "assert (\"abc\" in conv40up) == True\n",
    "assert (\"abc\" in conv20to40) == False\n",
    "assert (\"abc\" in conv0to20) == False\n",
    "\n",
    "# Print its properties\n",
    "print(obj4.group_storage.storage_[\"abc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessConversation(conv1, conv2):\n",
    "  a, b = conv1, conv2\n",
    "  if conv1[0] == \"'\" and conv1[-1] == \"'\":\n",
    "    a = conv1[1:-1]\n",
    "  if conv2[0] == \"'\" and conv2[-1] == \"'\":\n",
    "    b = conv2[1:-1]\n",
    "  return a,b\n",
    "\n",
    "def getUserAndCards(m):\n",
    "  m = m['original_text']\n",
    "  user_part, cards_part = m.split('&&')\n",
    "  user = user_part.split(',')\n",
    "  user.remove('SYSTEM')\n",
    "  user_part = ','.join(user)\n",
    "  return user_part, cards_part\n",
    "\n",
    "def generate_dataset(group_id_list):\n",
    "  a = []\n",
    "  for group_id in group_id_list:\n",
    "    conversations = list(delidata_corpus.corpus[group_id])\n",
    "    new_conversations = []\n",
    "    for m in conversations:\n",
    "      if m['message_type'] == \"INITIAL\":\n",
    "        users, cards = getUserAndCards(m)\n",
    "\n",
    "      if m['annotation_type'] == \"Probing\" or  m['annotation_type'] == \"Non-probing-deliberation\" or  m['message_type'] == \"SUBMIT\":\n",
    "        new_conversations.append(m)\n",
    "\n",
    "    conversation_dict = {}\n",
    "    count = 0\n",
    "    for i in range(1, len(new_conversations)-1):\n",
    "      if new_conversations[i]['original_text'] != '' and new_conversations[i+1]['original_text'] != '':\n",
    "\n",
    "        # the conversations sometimes are enclosed in inverted case so need to remove that\n",
    "        conv1, conv2 = preprocessConversation(new_conversations[i]['original_text'], new_conversations[i+1]['original_text'])\n",
    "        if count == 0:\n",
    "          conversation_dict[users] = cards\n",
    "          conversation_dict[cards] = conv1\n",
    "          count += 1\n",
    "        conversation_dict[conv1] = conv2\n",
    "    a.append(conversation_dict)\n",
    "\n",
    "  return a\n",
    "\n",
    "def export_to_csv(converstaion, name):\n",
    "  keys = []\n",
    "  values = []\n",
    "\n",
    "  for entry in converstaion:\n",
    "      for key, value in entry.items():\n",
    "          keys.append(key)\n",
    "          values.append(value)\n",
    "\n",
    "  df = pd.DataFrame({'Key': keys, 'Data': values})\n",
    "  csv_filename = f\"{name}.csv\"\n",
    "  df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = groups[:400]\n",
    "test_data = groups[400:450]\n",
    "eval_data = groups[450:]\n",
    "\n",
    "train_converstaion = generate_dataset(train_data)\n",
    "test_converstaion = generate_dataset(test_data)\n",
    "eval_converstaion = generate_dataset(eval_data)\n",
    "\n",
    "export_to_csv(train_converstaion, \"train\")\n",
    "export_to_csv(test_converstaion, \"test\")\n",
    "export_to_csv(eval_converstaion, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The result that is exported into the csv file is then uploaded into hugging face\n",
    "https://huggingface.co/datasets/apurbapokharel/deliconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate --upgrade\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 torchtext==0.14.1\n",
    "# pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset that i have preprocessed and uploaded in hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"apurbapokharel/deliconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_example(example, en_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens_1 = [token.text for token in en_nlp.tokenizer(example[\"Key\"])][:max_length]\n",
    "    en_tokens_2 = [token.text for token in en_nlp.tokenizer(example[\"Data\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens_1 = [token.lower() for token in en_tokens_1]\n",
    "        en_tokens_2 = [token.lower() for token in en_tokens_2]\n",
    "    en_tokens_1 = [sos_token] + en_tokens_1 + [eos_token]\n",
    "    en_tokens_2 = [sos_token] + en_tokens_2 + [eos_token]\n",
    "    return {\"conv1\": en_tokens_1, \"conv2\": en_tokens_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we need to create a Vocabulary to associate each token with a number, since our model will not get the words as an input but the index of the word from the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab_1 = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"conv1\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_index = en_vocab_1[unk_token]\n",
    "pad_index = en_vocab_1[pad_token]\n",
    "en_vocab_1.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab_1, en_vocab_2=None):\n",
    "    en_ids = en_vocab_1.lookup_indices(example[\"conv1\"])\n",
    "    # de_ids = en_vocab_2.lookup_indices(example[\"conv2\"])\n",
    "    de_ids = en_vocab_1.lookup_indices(example[\"conv2\"])\n",
    "    return {\"conv1_ids\": en_ids, \"conv2_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_kwargs = {\"en_vocab_1\": en_vocab_1, \"en_vocab_2\": en_vocab_2}\n",
    "fn_kwargs = {\"en_vocab_1\": en_vocab_1}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since model expects number in tensor format we convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"conv1_ids\", \"conv2_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, \n",
    "    columns=format_columns, \n",
    "    output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since, training is done in batches we create batch here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids_1 = [example[\"conv1_ids\"] for example in batch]\n",
    "        batch_en_ids_2 = [example[\"conv2_ids\"] for example in batch]\n",
    "        batch_en_ids_1 = nn.utils.rnn.pad_sequence(batch_en_ids_1, padding_value=pad_index)\n",
    "        batch_en_ids_2 = nn.utils.rnn.pad_sequence(batch_en_ids_2, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"conv1_ids\": batch_en_ids_1,\n",
    "            \"conv2_ids\": batch_en_ids_2,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "## generally 8,16,32,64 are used to create batch size\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab_1)\n",
    "output_dim = len(en_vocab_1)\n",
    "encoder_embedding_dim = 128\n",
    "decoder_embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"conv1_ids\"].to(device)\n",
    "        trg = batch[\"conv2_ids\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"conv1_ids\"].to(device)\n",
    "            trg = batch[\"conv2_ids\"].to(device)\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        test_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"tut1-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the prediction capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    en_vocab_1,\n",
    "    en_vocab_2,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = en_vocab_1.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        # print(tensor)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = en_vocab_2.lookup_indices([sos_token])\n",
    "        # print(inputs)\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab_2[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab_2.lookup_tokens(inputs)\n",
    "        # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_data[5][\"Key\"]\n",
    "expected_translation = train_data[5][\"Data\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    en_vocab_1,\n",
    "    en_vocab_1,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
